---
description: 
globs: *.py
alwaysApply: false
---
Project Rules: MCP Orchestrator Backend API
Project Mission (Prototype Phase)

The primary goal of this project is to build a robust, high-performance backend API that functions as an AI-driven orchestrator. This API will receive structured requests, use Google's Gemma model via Vertex AI to interpret user queries, fetch data from multiple backend MCP services, and return a synthesized JSON response.
This is a backend service. It does not serve any HTML or UI. It communicates exclusively via a JSON API to a separate frontend client.
Core Technology Stack
Language: Python 3.11+
Framework: FastAPI
Containerization: Docker
Deployment: Google Cloud Run
AI Engine: Google Vertex AI (for the Gemma model)
AI SDK: google-cloud-aiplatform library
Secrets Management: Google Secret Manager (for production)
Caching: Firestore (L1) and in-memory cachetools (L2)
Architecture & Service Communication
Microservices: The system consists of one main orchestrator service and multiple mcp-server microservices (e.g., chargebee, hubspot, firebase). Each service MUST run in its own Docker container.

API Contract:
The Orchestrator exposes a single primary public endpoint: POST /query.
Request Body (/query):
Generated json
{
    "query": "The user's natural language question",
    "email": "customer@example.com"
}
Use code with caution.
Json
Success Response Body (/query):
Generated json
{
    "answer": "The synthesized natural language answer from Gemma.",
    "sources": ["chargebee", "hubspot"],
    "confidence": 0.95,
    "cached": false,
    "timestamp": "2023-11-21T10:00:00Z"
}
Use code with caution.
Json
The MCP Servers expose a single internal endpoint: POST /mcp/call.
CORS Configuration (Critical for Prototype):
The main Orchestrator FastAPI application MUST be configured with CORSMiddleware.
For the prototype phase, allow_origins should be set to ["*"] to allow the local file-based frontend client to connect without issues. This will be locked down in production.
LLM Usage: Gemma via Vertex AI
SDK: All interactions with the LLM MUST use the google-cloud-aiplatform Python SDK. The openai library is not used in this project.
Authentication: The Orchestrator will use Application Default Credentials (ADC) to authenticate with the Vertex AI API. In local development, this is handled via gcloud auth application-default login. On Cloud Run, the service account's IAM permissions are used automatically.
Model: The target model is gemma-3-9b-it-e4b or a similar cost-effective Gemma variant. The model name MUST be configurable via an environment variable.
Prompts:
All LLM prompts MUST be stored in external files within the orchestrator/prompts/ directory (e.g., analysis.prompt, synthesis.prompt).
Prompts MUST use "defensive prompting" techniques: fence all user-provided input with XML tags (e.g., <query>{user_input}</query>) and include explicit instructions for the AI to prevent prompt injection.
Authentication & Authorization
PROTOTYPE PHASE: Authentication and authorization are intentionally deferred. The /query endpoint will be public and will not validate any tokens.
FUTURE: A placeholder comment _# TODO: Implement JWT validation_ should be added to the /query endpoint function to mark where security will be added later.
Caching Strategy (Multi-Layered)
A multi-layered cache is mandatory for performance and cost-efficiency.
L1: Orchestrator Intent Cache:
Location: orchestrator/orchestrator.py
Storage: Google Firestore.
Logic: Caches the final JSON response from the /query endpoint.
Key: MUST be a hash of the structured query_plan generated by the LLM's analysis step, not the raw query string.
L2: MCP Server Data Cache:
Location: In each individual MCP server (e.g., mcp-servers/chargebee/chargebee_mcp.py).
Storage: An in-memory, time-aware cache using the cachetools library (e.g., cachetools.TTLCache). This avoids adding the complexity of Redis for the prototype.
Logic: Caches the raw JSON data fetched from the downstream APIs (Chargebee, HubSpot, etc.).
Key: Based on the resource requested, typically the user's email (e.g., f"user_data:{email}").
Code Quality & Development Rules
Asynchronous I/O is Mandatory:
This is the most critical performance rule for all services.
Any blocking, synchronous function call (e.g., from chargebee, firebase-admin, or non-async SDKs) MUST be wrapped in await asyncio.to_thread().
Correct: user = await asyncio.to_thread(auth.get_user_by_email, email)
Incorrect: user = auth.get_user_by_email(email)
DRY (Don't Repeat Yourself):
A shared library at mcp-servers/common/ MUST be created.
This common library will provide Pydantic models (MCPRequest, MCPResponse) and a reusable FastAPI router or utility function for handling the /mcp/call endpoint. This avoids code duplication across MCP servers.
Configuration & Environment Variables:
All configuration (ports, API keys, project IDs, model names) MUST be loaded from environment variables using python-dotenv.
No hardcoded secrets or configuration values are allowed in the code.
A .env.example file must be maintained to show all required variables.
Dependency Management:
Each service MUST have its own requirements.txt.
Dependencies should be pinned to specific versions (==) to ensure reproducible builds.
Type Hinting & Formatting:
All Python code MUST include full type hints for function arguments and return values.
All code MUST be formatted using ruff format. Use ruff check for linting to maintain code quality.
Error Handling:
Handle specific exceptions where possible instead of using broad except Exception:.
Return structured JSON error messages to the client. In case of an error, the orchestrator should still return a 200 OK with a JSON body indicating the failure, e.g., { "error": "Could not retrieve data from HubSpot." }.
Log the full exception traceback on the server for debugging purposes.